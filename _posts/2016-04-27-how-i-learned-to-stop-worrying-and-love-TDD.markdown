---
title:  "How I Learned to Stop Worrying and Love TDD"
date:   2016-04-27 9:00:00 -0400
categories: jekyll update
color: cyan
layout: post
---
TDD can mean many things—from simple 'test first' practices focused mainly on integration or acceptance tests, all the way down to a highly granular, line-by-line, red-green-refactor methodology. I, personally, am not a TDD purist. I'll use TDD in some circumstances, and take a more relaxed approach in others. Generally speaking, the more concentrated and encapsulated the functionality the more likely I am to use TDD. Try to use TDD with highly diffuse code, and I'm likely to freeze up and suffer from design paralysis. I also picked up some poor testing practices by osmosis in my early years that I've had to mindfully and aggressively prune. The more I prune, the more effectively I find I can use TDD, and the more I find myself actually using TDD. In hindsight, a lot of my aversion to TDD over the years is traceable to my own bad habits and misconceptions—stumbling blocks and speedbumps that can slow the TDD process to a halt.

The simplest and first to go was the problem of not actually letting tests drive the design—rigidly imposing a design from the start and then expecting TDD to magically and readily produce a well-written, reliable implementation seems to be a common practice. Once it becomes obvious that different forms are more—or less—readily tested, it's a simple matter of making those patterns the default—composition over inheritance, dependency injection, encapsulation, minimal side-effects. This isn't the be-all-and-end-all, but not only is your code easier to test but almost as a direct consequence it's also much more well-designed. DHH might call this "design damage," but I call it "Testability Driven Design"—generally speaking, if your code isn't testable then it's badly designed, and if it's well designed then it will be easily tested. If you have to invent new disparaging terms to justify the poor testability of your code, well... good luck.

For me, the second wrong idea about testing to go was the intuition of the well-specified unit. I'm not sure how prevalent this is, but for the longest time I labored under the belief that each class should be a black box, and its interface tested exhaustively. It didn't matter if the class itself *had no logic of its own*, and simply incorporated functionality tested elsewhere. Taken to an extreme, this will be an obvious absurdity. The problem is that it often is taken to an extreme. Plenty of Rails devs will write tests that actually just exercise ActiveRecord, rather than their own code, in the belief that they need to exhaustively specify everything, right down to the automatically provided attribute readers. I believe the origin here lies in libraries which encourage much blurring of boundaries—the harder it can be to tell where app code ends and library code begins, the more one will instinctively 'play it safe' by over-testing.

This may be more or less controversial, but in my view a unit test should test only logic, and only logic *that is present in the unit itself*. "Logic" being code whose behaviour will vary depending on the input. Attribute accessors are not logic—simply testing for their presence is code duplication, and they should instead be exercised by higher-level tests (if they're not, then they're not used elsewhere in the code and so why do they exist?) Taking arguments and directly passing them to an injected dependency is not logic—that's glue code. Unit tests aren't black box tests—you don't have to suspend knowledge that certain functionality isn't actually implemented by the unit.

This leads to the third wrong idea, which is that the main purpose of a test is to prevent as many potential future bugs as possible. Preventing bugs is a *benefit* of testing, but it is not the purpose: well-designed, well-functioning, maintainable code is the purpose. Focusing on preventing bugs will lead directly to pathological testing, including code duplication, testing third party code, and their degenerate case: testing implementation detail. The classic example is the sort of tests encouraged by the 'shoulda' gem. If you're writing a unit test for an ActiveRecord model that ultimately asserts that a validation is present on the model by checking the model's validations array—please stop. You're just duplicating your code and tightly coupling your unit tests to third party code for zero reason. "But what if I accidentally delete that validation?" one might ask. Tests aren't there to verify that you wrote certain code—they're there to verify that the logic works correctly. Those aren't the same thing. If you want to verify a validation, then somewhere in your code it should be tested *by varying actual inputs.* If your test doesn't ultimately depend on some input somewhere changing, in all likelihood you're just duplicating your code, or testing someone else's code.

No doubt someone will disagree with my interpretation of TDD in light of the above, but once I started to shed these habits my willingness to use TDD and my velocity while doing so skyrocketed. I was no longer dreading the arduous task of writing tons of pointless tests just to make sure every line of code or potential 'contract' was covered. I no longer felt that adding a new class automatically meant a bunch of TDD boilerplate. Focusing all my tests on logic meant far fewer breakages when irrelevant glue code changed or low-value code was moved around. I still don't use TDD all the time, but I find myself shying away from using it out of fear of "design paralysis" less and less.